# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

#Load the dataset
df = pd.read_csv(r'C:\prodigyinfotech\task02\Mall_Customers.csv')  # Replace with the correct path to your file

# Data Preprocessing

# Check for missing values
print(df.isnull().sum())

# Drop rows with missing values (or handle them if necessary)
df.dropna(inplace=True)

#  Feature Selection
features = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
df_features = df[features]

# Feature Scaling
# Normalize the data to ensure features have a similar scale
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_features)

# Determine the optimal number of clusters (Elbow Method)
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(df_scaled)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow graph
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Apply K-means Clustering (Assuming 4 clusters based on Elbow method)
kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)
y_kmeans = kmeans.fit_predict(df_scaled)

#Add the cluster label to the original dataset
df['Cluster'] = y_kmeans

# Visualize the clusters using PCA (Principal Component Analysis)
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_scaled)

# Plotting the clusters in a 2D space
plt.figure(figsize=(10, 6))
sns.scatterplot(x=df_pca[:, 0], y=df_pca[:, 1], hue=df['Cluster'], palette='viridis')
plt.title('Customer Segments using K-means')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# Analyze the clusters
cluster_summary = df.groupby('Cluster').mean()
print(cluster_summary)
